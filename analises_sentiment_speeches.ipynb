{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94829fc1-4ec3-412b-9809-333b9685a8cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installing Required Python Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas==1.5.3\n",
    "%pip install openai\n",
    "%pip install transformers torch sklearn\n",
    "%pip install seaborn==0.11.2 matplotlib==3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97650161-49cc-41e6-9ff0-fa4b9844312a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Widget for User Input"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"tweet\", \"\")\n",
    "\n",
    "\n",
    "new_tweet = dbutils.widgets.get(\"tweet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b76a64-ba25-4c87-bf9c-5b9484edf27f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading Speech Data from CSV File"
    }
   },
   "outputs": [],
   "source": [
    "# CARREGAMENTO DE DADOS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_base = pd.read_csv(\n",
    "    '/Workspace/Users/flavio.assis@vale.com/TCC/left_right_speeches/speeches.csv',\n",
    "    sep=','\n",
    ")\n",
    "display(df_base.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed437b4-00da-408a-8e99-06a05d702a7f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering Texts by Token Count Using BERT Tokenizer"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Carregar o tokenizer do modelo BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Contar os tokens de cada texto\n",
    "df_base[\"num_tokens\"] = df_base[\"texto\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "\n",
    "\n",
    "df = df_base[(df_base[\"num_tokens\"] <= 256) & (df_base[\"num_tokens\"] >= 20)] #modelo 2, troquei de 128 =[12390,7552] para 20 = [16260,11162] = 27422\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c61d39-e9b0-4ada-ac38-f69dc95e0d80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Counting Frequency of Labels in DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d51918-e9d1-4068-be42-277bc1986f33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "sting ..."
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df[\"id\"] = range(29940, 29940 + len(df))\n",
    "\n",
    "\n",
    "# Load your DataFrame (assuming it's already loaded as df)\n",
    "df_0 = df[df[\"label\"] == 'Direita'].sample(n=4000, random_state=42) #reduzi o sample de direita\n",
    "df_1 = df[df[\"label\"] == 'Esquerda'].sample(n=6000, random_state=42)\n",
    "\n",
    "# Concatenate the two samples\n",
    "df_train = pd.concat([df_0, df_1])\n",
    "\n",
    "\n",
    "df_test = df[~df.index.isin(df_train.index)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('train:',df_train.info())\n",
    "print('test:',df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b66d7da-9a46-47a6-a898-b6a12b1889cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Displaying Test DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbdeecd-49a0-47dd-9ae3-d42b9387ba33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Counting Label Occurrences in Test DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc52934-093c-4eec-99a7-d3de032e2da1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Saving Train and Test DataFrames to Spark Tables"
    }
   },
   "outputs": [],
   "source": [
    "df_test_spark = spark.createDataFrame(df_test)\n",
    "df_train_spark = spark.createDataFrame(df_train)\n",
    "df_test_spark.write.mode(\"overwrite\").saveAsTable(\"db.analises_speeches_test_m2\")\n",
    "df_train_spark.write.mode(\"overwrite\").saveAsTable(\"db.analises_speeches_train_m2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1b44cd-cb8b-4445-95a4-81b28abc54c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clusterizando os tamanhos de textos"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE db.analises_speeches_results AS\n",
    "SELECT \n",
    "    *, \n",
    "    length(texto) as texto_length,\n",
    "    CASE \n",
    "        WHEN length(texto) BETWEEN 438 AND 650 THEN '438-650'\n",
    "        WHEN length(texto) BETWEEN 651 AND 900 THEN '651-900'\n",
    "        WHEN length(texto) BETWEEN 901 AND 1150 THEN '901-1150'\n",
    "        WHEN length(texto) BETWEEN 1151 AND 1336 THEN '1151-1336'\n",
    "        ELSE 'Out of range'\n",
    "    END AS texto_length_group\n",
    "FROM db.analises_speeches_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7903f0d2-fe63-43b9-b2ef-cf0071fecefe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading Speech Analysis Data into Pandas DataFrames"
    }
   },
   "outputs": [],
   "source": [
    "df_test = spark.table(\"db.analises_speeches_test_m2\").toPandas()\n",
    "df_train = spark.table(\"db.analises_speeches_train_m2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6266cf80-051d-4a15-a650-0235a845f317",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Displaying Info for Train and Test DataFrames"
    }
   },
   "outputs": [],
   "source": [
    "print(df_test.info())\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "266faf1a-800e-4410-8b79-1de126ea5c59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating and Managing Speech Label Classifier"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "class SpeechLabelClassifier:\n",
    "    def __init__(self, model_name='neuralmind/bert-base-portuguese-cased', \n",
    "                 examples_per_category=5, \n",
    "                 llm_model=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "                 save_path=\"/Workspace/Users/flavio.assis@vale.com/TCC/left_right_speeches/\"):\n",
    "        # Model configuration\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.examples_per_category = examples_per_category\n",
    "        self.embeddings_cache = {}\n",
    "        self.reference_examples = None\n",
    "        self.reference_embeddings = None\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        # LLM configuration\n",
    "        self.llm_client = OpenAI(\n",
    "            api_key=\"SECRET_DB_KEY\",\n",
    "            base_url=\"URL_DB\"\n",
    "        )\n",
    "        self.llm_model = llm_model\n",
    "        # Create directory if it doesn't exist\n",
    "        dbutils.fs.mkdirs(f\"dbfs:{save_path}\")\n",
    "        # Keyword dictionary for label categories\n",
    "       \n",
    "        print(f\"Embeddings will be saved to: {self.save_path}\")\n",
    "        \n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"Generates embedding for a text with proper error handling\"\"\"\n",
    "        try:\n",
    "            if not isinstance(text, str):\n",
    "                print(f\"Warning: Received non-string input: {type(text)}\")\n",
    "                text = str(text)\n",
    "                \n",
    "            \n",
    "            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).numpy()[0]\n",
    "                #print(f\"Successfully generated embedding of shape: {embedding.shape}\")\n",
    "                return embedding\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_embedding: {str(e)}\")\n",
    "            print(f\"Failed text: {text}\")\n",
    "            raise  # Re-raise the exception to handle it in prepare_reference_data\n",
    "\n",
    "    def save_embeddings(self, filename='speechs_embeddings.pkl'):\n",
    "        \"\"\"Saves embeddings and reference examples\"\"\"\n",
    "        try:\n",
    "            if self.reference_examples is None or self.reference_embeddings is None:\n",
    "                print(\"No embeddings to save - reference data is None\")\n",
    "                return\n",
    "                \n",
    "            # Construct full Databricks path\n",
    "            full_path = os.path.join(self.save_path, filename)\n",
    "            \n",
    "            data = {\n",
    "                'reference_examples': self.reference_examples,\n",
    "                'reference_embeddings': self.reference_embeddings\n",
    "            }\n",
    "            \n",
    "            # Save using Databricks file system\n",
    "            with open(full_path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"Embeddings saved successfully to {full_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error saving embeddings: {str(e)}')\n",
    "            \n",
    "    def load_embeddings(self, filename='speechs_embeddings.pkl'):\n",
    "        \"\"\"Loads saved embeddings and examples\"\"\"\n",
    "        full_path = os.path.join(self.save_path, filename)\n",
    "        with open(full_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.reference_examples = data['reference_examples']\n",
    "        self.reference_embeddings = data['reference_embeddings']\n",
    "    \n",
    "    def prepare_reference_data(self, df, force_compute=True, filename='speechs_embeddings.pkl'):\n",
    "        \"\"\"Prepares reference data with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            print(\"\\nStarting prepare_reference_data...\")\n",
    "            print(f\"Input DataFrame shape: {df.shape}\")\n",
    "            print(f\"Columns available: {df.columns.tolist()}\")\n",
    "            \n",
    "            full_path = os.path.join(self.save_path, filename)\n",
    "            print(f\"Working with path: {full_path}\")\n",
    "            \n",
    "           \n",
    "            if not force_compute and os.path.exists(full_path):\n",
    "                print(\"Loading saved embeddings...\")\n",
    "                self.load_embeddings(filename)\n",
    "                return self.reference_examples\n",
    "            \n",
    "            print(\"\\nComputing new embeddings...\")\n",
    "            if df is None or len(df) == 0:\n",
    "                raise ValueError(\"Input DataFrame is empty or None\")\n",
    "                \n",
    "            representative_examples = []\n",
    "            \n",
    "            # Print unique label for debugging\n",
    "            unique_label = df['label'].unique()\n",
    "            print(f\"Unique label values found: {unique_label}\")\n",
    "            \n",
    "            for label in unique_label:\n",
    "                print(f\"\\nProcessing label: {label}\")\n",
    "                label_df = df[df['label'] == label]\n",
    "                print(f\"Number of examples for label {label}: {len(label_df)}\")\n",
    "                \n",
    "                embeddings = []\n",
    "                successful_indices = []\n",
    "                \n",
    "                for idx, row in label_df.iterrows():\n",
    "                    try:\n",
    "                        texto = row['texto']\n",
    "                        embedding = self.get_embedding(texto)\n",
    "                        embeddings.append(embedding)\n",
    "                        successful_indices.append(idx)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to process row {idx}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                if not embeddings:\n",
    "                    print(f\"Warning: No successful embeddings for label {label}\")\n",
    "                    continue\n",
    "                    \n",
    "                embeddings = np.array(embeddings)\n",
    "                print(f\"Generated {len(embeddings)} embeddings for label {label}\")\n",
    "                \n",
    "                if len(embeddings) <= self.examples_per_category:\n",
    "                    for idx in successful_indices:\n",
    "                        representative_examples.append(label_df.loc[idx].to_dict())\n",
    "                else:\n",
    "                    kmeans = KMeans(n_clusters=self.examples_per_category, random_state=42)\n",
    "                    kmeans.fit(embeddings)\n",
    "                    \n",
    "                    for centroid in kmeans.cluster_centers_:\n",
    "                        distances = np.linalg.norm(embeddings - centroid, axis=1)\n",
    "                        closest_idx = distances.argmin()\n",
    "                        actual_idx = successful_indices[closest_idx]\n",
    "                        representative_examples.append(label_df.loc[actual_idx].to_dict())\n",
    "            \n",
    "            print(f\"\\nTotal representative examples gathered: {len(representative_examples)}\")\n",
    "            \n",
    "            if not representative_examples:\n",
    "                raise ValueError(\"No representative examples were generated\")\n",
    "                \n",
    "            self.reference_examples = pd.DataFrame(representative_examples)\n",
    "            print(f\"Created reference examples DataFrame with shape: {self.reference_examples.shape}\")\n",
    "            \n",
    "            self.reference_embeddings = np.array([\n",
    "                self.get_embedding(text) \n",
    "                for text in self.reference_examples['texto']\n",
    "            ])\n",
    "            print(f\"Created reference embeddings array with shape: {self.reference_embeddings.shape}\")\n",
    "            \n",
    "            # Save the computed embeddings\n",
    "            self.save_embeddings(filename)\n",
    "            print(f\"Embeddings computation completed and saved to {full_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error in prepare_reference_data: {str(e)}')\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            raise\n",
    "            \n",
    "        return self.reference_examples\n",
    "    \n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    def find_matching_keywords(self, text):\n",
    "        \"\"\"Finds matching keywords in the text for each label category\"\"\"\n",
    "        matches = {}\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for label, keywords in self.label_keywords.items():\n",
    "            label_matches = []\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in text_lower:\n",
    "                    label_matches.append(keyword)\n",
    "            if label_matches:\n",
    "                matches[label] = label_matches\n",
    "                \n",
    "        return matches\n",
    "    \n",
    "    def find_similar_examples(self, new_speech, top_k=5):\n",
    "        \"\"\"Finds the most similar examples to the new tweet\"\"\"\n",
    "        new_embedding = self.get_embedding(new_speech)\n",
    "        similarities = cosine_similarity([new_embedding], self.reference_embeddings)[0]\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        similar_examples = []\n",
    "        for idx in top_indices:\n",
    "            similar_examples.append({\n",
    "                'speech': self.reference_examples.iloc[idx]['texto'],\n",
    "                'label': self.reference_examples.iloc[idx]['label'],\n",
    "                'similarity': similarities[idx]\n",
    "            })\n",
    "            \n",
    "        return similar_examples\n",
    "    \n",
    "    def create_classification_prompt(self, new_speech, similar_examples):\n",
    "        \"\"\"Creates an optimized prompt for LLM using similar examples and keywords\"\"\"\n",
    "        prompt = \"Você é um especialista em análise de discursos em português de políticos, para definir se a orientação política deles é de Esquerda ou Direita.\\n\\n\"\n",
    "        prompt += \"Categorias disponíveis:\\n\"\n",
    "        prompt += \"Esquerda ou Direita\\n\\n\"\n",
    "\n",
    "        prompt += \"Exemplos similares ao novo discurso:\\n\\n\"\n",
    "        for ex in similar_examples:\n",
    "            prompt += f\"Discurso: {ex['speech']}\\n\"\n",
    "            prompt += f\"Orientação política: {ex['label']}\\n\"\n",
    "            prompt += f\"Similaridade: {ex['similarity']:.2f}\\n\\n\"\n",
    "        \n",
    "        prompt += f\"Novo discurso para classificar:\\n{new_speech}\\n\\n\"\n",
    "        prompt += \"Instruções específicas:\\n\"\n",
    "        prompt += \"1. Analise o contexto geral do discurso\\n\"\n",
    "        prompt += \"2. Compare com os exemplos similares fornecidos\\n\"\n",
    "        prompt += \"3. Considere o que você entende de políticas de Direita e Esquerda\\n\\n\"\n",
    "        prompt += \"Classifique o discurso com Direita ou Esquerda.\"\n",
    "        prompt += \"Responda apenas com categorias Direita ou Esquerda.\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def classify_speech(self, new_speech):\n",
    "        \"\"\"Classifies a new tweet using embeddings + LLM\"\"\"\n",
    "        \n",
    "        similar_examples = self.find_similar_examples(new_speech, top_k=5)\n",
    "        \n",
    "        prompt = self.create_classification_prompt(new_speech, similar_examples)\n",
    "        \n",
    "        \n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Você é um especialista em análise de discursos em português de políticos, para definir se a orientação política deles é de Esquerda ou Direita\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=self.llm_model,\n",
    "            max_tokens=500,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        classification = response.choices[0].message.content.strip()\n",
    "        #print(similar_examples)\n",
    "        return {\n",
    "            'predict_label': classification,\n",
    "            'similar_examples': similar_examples\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fc21bb-75e4-43fa-872b-b7c59e7ca6a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Classifying and Storing Speech Analysis Results"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def run_speech_analysis(df_test, classifier, max_retries=6, sleep_time=600):\n",
    "    # Initialize empty lists to store results\n",
    "    ids = []\n",
    "    speeches = []\n",
    "    i = 0\n",
    "   \n",
    "    # Process each tweet\n",
    "    for _, row in df_test.iterrows():\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                \n",
    "                result = classifier.classify_speech(row['texto'])\n",
    "                \n",
    "                new_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": [str(row['id'])],\n",
    "                        \"texto\": [str(row['texto'])],\n",
    "                        \"label\": [str(row['label'])],\n",
    "                        \"predict_label\": [str(result['predict_label'])],\n",
    "                        \"model\": [\"2.0\"],\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                \n",
    "                spark_new_row = spark.createDataFrame(new_row)\n",
    "                \n",
    "            # Step 2: Append the new row to the Delta table\n",
    "                spark_new_row.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "                    \"db.analises_speeches_results\"\n",
    "                )\n",
    "               \n",
    "               \n",
    "                i += 1\n",
    "                print(f\"Processed {str(row['id'])} , {i}\")\n",
    "                break  # Exit retry loop on success\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying {retries + 1}/{max_retries}...\")\n",
    "                print(str(row['id']))\n",
    "                retries += 1\n",
    "                time.sleep(sleep_time)  # Wait before retrying\n",
    "        else:\n",
    "            print(f\"Skipping tweet ID {row['id']} after {max_retries} failed attempts.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe8dbec-e0a2-47a1-9eba-edd52ff94d94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering Missing IDs from Test Data Using SQL"
    }
   },
   "outputs": [],
   "source": [
    "df_results = spark.sql(\"SELECT id FROM db.analises_speeches_results WHERE model = '2.0'\").toPandas()\n",
    "df_missing_ids = df_test[~df_test['id'].isin(df_results['id'].astype(int))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5155fb44-43e2-4243-a4ee-b818c3c75842",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Counting Rows in Test, Missing, and Results DataFrames"
    }
   },
   "outputs": [],
   "source": [
    "print('test',df_test.count())\n",
    "print('missing:',df_missing_ids.count())\n",
    "print('results:',df_results.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc653b88-bb41-4a23-b39d-7d3188f45d3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing Classifier and Running Speech Analysis"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# Initialize classifier\n",
    "classifier = SpeechLabelClassifier()\n",
    "\n",
    "# First, prepare the reference data using training data\n",
    "classifier.prepare_reference_data(df_train)  # Using your training dataset\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "run_speech_analysis(df_missing_ids, classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d027578-f3eb-4bfc-9acb-fb1696241af8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comparing Predicted and Actual Labels in Speech Results"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *, predict_label = label as match from db.analises_speeches_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41ab24a-eb37-4e57-b413-aab3a2204ef2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grouping Text Lengths and Updating Analysis Results"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE db.analises_speeches_results\n",
    "SET texto_length_group = \n",
    "    CASE \n",
    "        WHEN length(texto) BETWEEN 50 AND 250 THEN '50-250'\n",
    "        WHEN length(texto) BETWEEN 251 AND 500 THEN '251-500'\n",
    "        WHEN length(texto) BETWEEN 501 AND 750 THEN '501-750'\n",
    "        WHEN length(texto) BETWEEN 751 AND 1000 THEN '751-1000'\n",
    "        WHEN length(texto) BETWEEN 1001 AND 1250 THEN '1001-1250'\n",
    "        WHEN length(texto) BETWEEN 1251 AND 1500 THEN '1251-1500'\n",
    "        ELSE 'out_of_range'\n",
    "    END\n",
    "WHERE model = \"2.0\" AND predict_label IN ('Direita', 'Esquerda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4944341d-f370-40ee-b630-4a4cc8125115",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing Model Accuracy by Text Length and Labels"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    texto_length_group,\n",
    "    (SUM(CASE WHEN predict_label = label THEN 1 ELSE 0 END) / COUNT(*)) * 100 AS percent_true,\n",
    "    COUNT(*) AS count_total,\n",
    "    COUNT(CASE WHEN predict_label = label THEN 1 END) as count_true,\n",
    "    (SUM(CASE WHEN predict_label = label AND label = 'Direita' THEN 1 ELSE 0 END) /\n",
    "    NULLIF(COUNT(CASE WHEN label = 'Direita' THEN 1 END), 0)) * 100 AS percent_true_direita,\n",
    "    COUNT(CASE WHEN label = 'Direita' THEN 1 END) AS count_direita,\n",
    "    COUNT(CASE WHEN predict_label = label AND label = 'Direita' THEN 1 END) as count_direita_true,\n",
    "    (SUM(CASE WHEN predict_label = label AND label = 'Esquerda' THEN 1 ELSE 0 END) /\n",
    "    NULLIF(COUNT(CASE WHEN label = 'Esquerda' THEN 1 END), 0)) * 100 AS percent_true_esquerda,\n",
    "    COUNT(CASE WHEN label = 'Esquerda' THEN 1 END) AS count_esquerda,\n",
    "    COUNT(CASE WHEN predict_label = label AND label = 'Esquerda' THEN 1 END) as count_esquerda_true\n",
    "FROM db.analises_speeches_results\n",
    "WHERE model = \"2.0\" AND predict_label IN ('Direita', 'Esquerda')\n",
    "GROUP BY texto_length_group;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e43dac0-a4e3-4697-80b5-5edac9b959bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing Model Predictions by Text Length and Label"
    }
   },
   "outputs": [],
   "source": [
    "# SQL query to get the required data\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    texto_length_group,\n",
    "    (SUM(CASE WHEN predict_label = label THEN 1 ELSE 0 END) / COUNT(*)) * 100 AS percent_true,\n",
    "    COUNT(*) AS count_total,\n",
    "    COUNT(CASE WHEN predict_label = label THEN 1 END) as count_true,\n",
    "    (SUM(CASE WHEN predict_label = label AND label = 'Direita' THEN 1 ELSE 0 END) /\n",
    "    NULLIF(COUNT(CASE WHEN label = 'Direita' THEN 1 END), 0)) * 100 AS percent_true_direita,\n",
    "    COUNT(CASE WHEN label = 'Direita' THEN 1 END) AS count_direita,\n",
    "    COUNT(CASE WHEN predict_label = label AND label = 'Direita' THEN 1 END) as count_direita_true,\n",
    "    (SUM(CASE WHEN predict_label = label AND label = 'Esquerda' THEN 1 ELSE 0 END) /\n",
    "    NULLIF(COUNT(CASE WHEN label = 'Esquerda' THEN 1 END), 0)) * 100 AS percent_true_esquerda,\n",
    "    COUNT(CASE WHEN label = 'Esquerda' THEN 1 END) AS count_esquerda,\n",
    "    COUNT(CASE WHEN predict_label = label AND label = 'Esquerda' THEN 1 END) as count_esquerda_true\n",
    "FROM db.analises_speeches_results\n",
    "WHERE model = '2.0' AND predict_label IN ('Direita', 'Esquerda')\n",
    "GROUP BY texto_length_group\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and get the results\n",
    "df_results = spark.sql(query)\n",
    "display(df_results)\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_matrix_query = \"\"\"\n",
    "SELECT \n",
    "    label,\n",
    "    predict_label,\n",
    "    COUNT(*) AS count\n",
    "FROM db.analises_speeches_results\n",
    "WHERE model = '2.0' AND predict_label IN ('Direita', 'Esquerda')\n",
    "GROUP BY label, predict_label\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and get the confusion matrix\n",
    "df_confusion_matrix = spark.sql(confusion_matrix_query)\n",
    "display(df_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f47b95e-dbcc-485d-844a-5fb9df8383d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing Prediction Accuracy for Different Labels"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    (SUM(CASE WHEN predict_label = label THEN 1 ELSE 0 END) / COUNT(*)) * 100 AS percent_true,\n",
    "    COUNT(*) AS count_total,\n",
    "    COUNT(CASE WHEN predict_label = label THEN 1 END) as count_true,\n",
    "    (SUM(CASE WHEN predict_label = label AND label = 'Direita' THEN 1 ELSE 0 END) /\n",
    "    NULLIF(COUNT(CASE WHEN label = 'Direita' THEN 1 END), 0)) * 100 AS percent_true_direita,\n",
    "    COUNT(CASE WHEN label = 'Direita' THEN 1 END) AS count_direita,\n",
    "    COUNT(CASE WHEN predict_label = label AND label = 'Direita' THEN 1 END) as count_direita_true,\n",
    "    (SUM(CASE WHEN predict_label = label AND label = 'Esquerda' THEN 1 ELSE 0 END) /\n",
    "    NULLIF(COUNT(CASE WHEN label = 'Esquerda' THEN 1 END), 0)) * 100 AS percent_true_esquerda,\n",
    "    COUNT(CASE WHEN label = 'Esquerda' THEN 1 END) AS count_esquerda,\n",
    "    COUNT(CASE WHEN predict_label = label AND label = 'Esquerda' THEN 1 END) as count_esquerda_true\n",
    "    \n",
    "    \n",
    "    \n",
    "FROM db.analises_speeches_results\n",
    "WHERE model = \"2.0\" and predict_label in ('Direita','Esquerda') ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2ee275-ccf8-4385-b09f-ff71a898efd1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculating and Displaying Speech Model Metrics"
    }
   },
   "outputs": [],
   "source": [
    "# Display results in the requested format\n",
    "results = spark.sql(\"SELECT * FROM db.analises_speeches_results where model = '2.0' and predict_label in ('Direita','Esquerda')\").toPandas()\n",
    "display(results)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(results['label'], results['predict_label'])\n",
    "precision = precision_score(results['label'], results['predict_label'], average='weighted')\n",
    "recall = recall_score(results['label'], results['predict_label'], average='weighted')\n",
    "f1 = f1_score(results['label'], results['predict_label'], average='weighted')\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Acurácia: {accuracy:.4f}\")\n",
    "print(f\"Precisão: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c726234-5875-41a4-9895-ca201e5f2472",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculating Metrics for Direita Speech Analysis Model"
    }
   },
   "outputs": [],
   "source": [
    "results_direita = spark.sql(\"SELECT * FROM db.analises_speeches_results where label =  'Direita' and model = '2.0'\").toPandas()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(results_direita['label'], results_direita['predict_label'])\n",
    "precision = precision_score(results_direita['label'], results_direita['predict_label'], average='weighted')\n",
    "recall = recall_score(results_direita['label'], results_direita['predict_label'], average='weighted')\n",
    "f1 = f1_score(results_direita['label'], results_direita['predict_label'], average='weighted')\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Acurácia: {accuracy:.4f}\")\n",
    "print(f\"Precisão: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78028f3-9974-43da-9c3b-6b9a210f1690",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculating Evaluation Metrics for Esquerda Model 2.0"
    }
   },
   "outputs": [],
   "source": [
    "results_esquerda = spark.sql(\"SELECT * FROM db.analises_speeches_results where label =  'Esquerda' and model = '2.0'\").toPandas()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(results_esquerda['label'], results_esquerda['predict_label'])\n",
    "precision = precision_score(results_esquerda['label'], results_esquerda['predict_label'], average='weighted')\n",
    "recall = recall_score(results_esquerda['label'], results_esquerda['predict_label'], average='weighted')\n",
    "f1 = f1_score(results_esquerda['label'], results_esquerda['predict_label'], average='weighted')\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Acurácia: {accuracy:.4f}\")\n",
    "print(f\"Precisão: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e30092-6dfd-4b56-a3b6-60cf44f9f734",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Categorizing Text Lengths into Groups"
    }
   },
   "outputs": [],
   "source": [
    "results['texto_length'] = results['texto'].apply(len)\n",
    "\n",
    "def length_group(length):\n",
    "    if 50 <= length <= 250:\n",
    "        return '50-250'\n",
    "    elif 251 <= length <= 500:\n",
    "        return '251-500'\n",
    "    elif 501 <= length <= 750:\n",
    "        return '501-750'\n",
    "    elif 751 <= length <= 1000:\n",
    "        return '751-1000'\n",
    "    elif 1001 <= length <= 1250:\n",
    "        return '1001-1250'\n",
    "    elif 1251 <= length <= 1500:\n",
    "        return '1251-1500'\n",
    "    else:\n",
    "        return 'out_of_range'\n",
    "\n",
    "results['texto_length_group'] = results['texto_length'].apply(length_group)\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b317fc9-5dcd-4ffc-bd02-5a4ab3a3a130",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualizing Confusion Matrix with Heatmap"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(results['label'], results['predict_label'])\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Direita', 'Esquerda'], yticklabels=['Direita', 'Esquerda'])\n",
    "plt.xlabel(\"Predito\")\n",
    "plt.ylabel(\"Verdadeiro\")\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f709ae17-f618-47b2-a848-ab9f5db2d492",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualizing Confusion Matrix by Text Length Group"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create confusion matrix for each texto_length_group\n",
    "for group in results['texto_length_group'].unique():\n",
    "    group_results = results[results['texto_length_group'] == group]\n",
    "    cm = confusion_matrix(group_results['label'], group_results['predict_label'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Direita', 'Esquerda'], yticklabels=['Direita', 'Esquerda'])\n",
    "    plt.xlabel(\"Predito\")\n",
    "    plt.ylabel(\"Verdadeiro\")\n",
    "    plt.title(f\"Matriz de Confusão - Grupo de Comprimento de Texto: {group}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87f27ca-affd-41ff-9c14-03e963c8d410",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Plotting Accuracy by Text Length Group and Label"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the percentage of correct predictions for each label and texto_length_group\n",
    "percentages = results.groupby(['texto_length_group', 'label']).apply(lambda x: (x['predict_label'] == x['label']).mean()).reset_index()\n",
    "percentages.columns = ['texto_length_group', 'label', 'percent_true']\n",
    "\n",
    "# Pivot the data for easier plotting\n",
    "percentages_pivot = percentages.pivot(index='texto_length_group', columns='label', values='percent_true')\n",
    "\n",
    "# Define the order of the x-axis\n",
    "order = ['50-250', '251-500', '501-750', '751-1000', '1001-1250', '1251-1500']\n",
    "percentages_pivot = percentages_pivot.reindex(order)\n",
    "\n",
    "# Plot the line chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "for label in percentages_pivot.columns:\n",
    "    plt.plot(percentages_pivot.index, percentages_pivot[label], marker='o', label=label)\n",
    "\n",
    "plt.xlabel('Texto Length Group')\n",
    "plt.ylabel('Percent of True Predictions')\n",
    "plt.title('Percent of True Predictions by Texto Length Group and Label')\n",
    "plt.legend(title='Label')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f81156-f778-46c4-937b-871c1df1ade7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualizing Confusion Matrices by Length Group"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a single figure for all confusion matrices\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(results['texto_length_group'].unique()), figsize=(20, 7))\n",
    "\n",
    "# Create confusion matrix for each texto_length_group\n",
    "for ax, group in zip(axes, results['texto_length_group'].unique()):\n",
    "    group_results = results[results['texto_length_group'] == group]\n",
    "    cm = confusion_matrix(group_results['label'], group_results['predict_label'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Direita', 'Esquerda'], yticklabels=['Direita', 'Esquerda'], ax=ax)\n",
    "    ax.set_xlabel(\"Predito\")\n",
    "    ax.set_ylabel(\"Verdadeiro\")\n",
    "    ax.set_title(f\"Grupo: {group}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7970065914169041,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "analises_sentiment_speeches",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
